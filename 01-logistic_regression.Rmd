---
title: "Cool Title"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
---

Standard libraries
```{r}
rm(list = ls())
options(scipen=999) # scientific notation

library(data.table)
library(ggplot2)
library(caret)
library(plotly)
library(dplyr)
library(DT)
library(pander)
library(plotly)
library(xts)
library(reshape2)
library(arules)
library(glmnet)
library(mlbench)
library(pROC)
library(caTools)
library(ranger)
library(gains)

#devtools::install_github("luisalejandrolee/utilucho")
library(utilucho)

#source("scripts/data_cleaning_and_preparation.R")
source("scripts/models_measures.R")
```

# Import

```{r}
# File names contain the sample size used. Need for the import (must match
# a run of the preparation file to save files with this sample)
sample_size <- 5000
#sample_size <- 100000
lags <- 2

folder <- "data/staging/"
file_dt <- paste0("master_n", sample_size , "_lags", max(lags), ".csv")
dt <- fread(paste0(folder, file_dt))

```


# Remove some variables
Due to collineatiry. Particularly, the "diff", "new" and " loss" which are already captured
by the "lag_1" variables. Careful here, because one must keep some "loss" or "new"
variables intended to be the target
```{r}
# target to keep (so exclude from general removal of variables below)
target_var <- "t_credito_new"

# needed to use the caret functions ( 0 and 1 are not possible variable names,
# and caret needs the levels in the factors to be possible variable names)
dt[, (target_var) := ifelse(get(target_var) == 1, "yes", "no")]

# manual removal. Key here to avoid the lag_1 of the target var (perfect collinearity)
cols_manual <- c()
#cols_manual <- "t_credito_lag_1"

# remove all variables that by construction will probably have perfecto collinearity
# with other regressors
cols_diff <- grep('_diff', names(dt), value = TRUE)
cols_new <- grep('_new', names(dt), value = TRUE)
cols_loss <- grep('_loss', names(dt), value = TRUE)

# put all groups together, but avoid removing the target_var
cols_remove <- c(cols_diff, cols_new, cols_loss, cols_manual)
cols_remove <- cols_remove[!(cols_remove %in% target_var)]

# finally remove the chosen columns
dt[, (cols_remove) := NULL]

#cols_remove
#names(dt)
```


# Missing values

```{r}
# remove all na's
dt <- na.omit(dt)
```

# Train and test split

```{r}
# columns not to be included in the regression
to_exclude <- c("V1", "periodo", "num_id", "nomina", "t_credito", "ahorros",
                "corriente", "crediservice", "cdt", "ordinario", "libranza", "libredestino",
                "otros", "vehiculo", "vivienda", "fomento", "microcredito", "leasing",
                "activo_pyme", "constructor")

to_exclude <- c("V1", "periodo", "num_id", "t_credito")

all_cols <- names(dt)[!(names(dt) %in% to_exclude)]
# target variable
y_cols <- target_var
dt[, (y_cols) := lapply(.SD, as.factor), .SDcols = y_cols] # convert to factor

# all predictor variables (all except the target)
x_cols <- all_cols[!(all_cols %in% y_cols)]

#length(names(dt))
#length(all_cols)
#length(x_cols)
#length(y_cols)

# months to use as test
test_months <- c("2018-06-30")
# train months are all but the ones chosen for test
months <- unique(dt$periodo) # all months in data
train_months <- months[!(months %in% test_months)] # all months but for test

# notice that splits below are not done randomly. This is due to having time
# series variables. If done randomly, one would have the model observing data of
# future purchases to predict past ones (for example, client A in August for training,
# and same client A in July for testing)

# complete training and test set (including x and y), used for traditional logit regression
train_full <- dt[periodo %in% train_months, .SD, .SDcols = all_cols]
test_full <- dt[periodo %in% test_months, .SD, .SDcols = all_cols]

# separate predictors (x) vs target (y) for lasso (glmnet) regression
train_x <- dt[periodo %in% train_months, .SD, .SDcols = x_cols]
train_y <- dt[periodo %in% train_months, lapply(.SD, as.factor), .SDcols = y_cols]

# test split: x for explanatory, y for target
test_x <- dt[periodo %in% test_months, .SD, .SDcols = x_cols]
test_y <- dt[periodo %in% test_months, lapply(.SD, as.factor), .SDcols = y_cols]

#ncol(train_full)
#train_x
#train_y

#test_x
#test_y
```

```{r}
train_full[t_credito_new == "1", .(t_credito_new)]
train_full
```

# Logistic regression

## Fit model

```{r}

fit1_logit <- glm(data = train_full, t_credito_new ~., family = "binomial",
                  control = list(maxit = 50))

summary(fit1_logit)
```
## Confusion matrix, ROC curve and AUC
```{r}
#predict probabilities on testset
#type=”response” gives probabilities, type=”class” gives class
glm_prob <- predict.glm(object = fit1_logit, newdata = test_x, type = "response")
#summary(glm_prob)
#make classifications predictions based on probability

## threshold on probability to decide on class
glm_predict <- ifelse(glm_prob > 0.5, "yes", "no")

# use caret to do both the confusion matrix and print some stats
confusionMatrix(as.factor(glm_predict), test_y$t_credito_new, positive = "yes")

# use caTools package to plot ROC curve
colAUC(glm_prob, test_y$t_credito_new, plotROC = TRUE)


#test_y[t_credito_new==1]
#glm_predict
#glm_predict == test_y$t_credito_new
```
## Conclusions on traditional logistic

The model is highly unbalanced making the use of other models necessary.

The naive prediction (no one buys a credit card) gives over 99 percent accuracy 
(the "No information Rate" measure)

One could calculate the AUC, but is generally not so useful for just one model. The
AUC is more useful for comparing models, selecting among them when tuning parameters.
Hence, usually what is found for AUC uses caret's trainControl object, to try different
models.


Next steps:

* Try other models to deal with unbalanced class
  * LASSO and Random Forest (datacamp has the course in that same order)
  
  * Try cross validation (use trainControl) to select regularization term.
  I don't trust it much (since I have time series) but worth trying and learning from it

# Random forest

Notice that the logistic regression is simple in the sense that it doesn't have
any hyperparametres to tune. The next model would be a random forest, for which the
main parameter is "mtry" (number of explanatory variables to randomly try at each split 
of each tree). Caret chooses this parameter automatically. The other parameter is 
the length of the tree (but caret doesn't do it automatically)

## Simple cross validation, auto tuning
mtry is by default chosen in caret (tuneLength = 3  means try 3 different values
for mtry, and choose the best one)
```{r}
#train_full$t_credito_new <- ifelse(train_full$t_credito_new == "0", "no", "yes")

# Fit random forest: model
## tuneLenght auto tunes default parameters for chosen model. For random forest,
## it tunes mtry (number of features randomly chosen for growing each tree)
## twoClassSummary indicates using AUC for model selection (vs. accuracy which is default)
randfor_model_default <- train(
  t_credito_new ~.,
  tuneLength = 2,
  data = train_full, method = "ranger",
  trControl = trainControl(method = "cv", number = 2, verboseIter = TRUE, 
                           summaryFunction = twoClassSummary, classProbs = TRUE)
)
```
## Save the model
```{r}
models_path <- "data/models/"
model_name <- "rforest_default_tuning_3.rds"
#saveRDS(randfor_model_default, paste0(models_path, model_name))
```

## Load model
```{r}
models_path <- "data/models/"
model_name <- "rforest_default_tuning_3.rds"
#randfor_model_default <- readRDS(paste0(models_path, model_name))

```

## Check model
```{r}
# Print model to console
print(randfor_model_default)

# Plot model
plot(randfor_model_default)

#predict probabilities on testset
#type=”response” gives probabilities, type=”class” gives class
probs_rforest <- predict(object= randfor_model_default, newdata = test_x, type = "prob")

# plot ROC curve
roc_rforest <- roc(test_y$t_credito_new, probs_rforest$yes)
plot(roc_rforest)

# plot best threshold and accuracy
result_coords <- coords(roc_rforest, "best", best.method="closest.topleft",
                        ret=c("threshold", "accuracy"))
print(result_coords)#to get threshold and accuracy

## uplift and gains
# gets a table with relevant data
lift_data_rforest <- calculate_lift_table(depvar = ifelse(test_y$t_credito_new == "yes", 1, 0),
                                  predcol = probs_rforest$yes)

# print bars
temp <- lift_data_rforest %>% melt(id.vars = c("bucket"))
temp <- as.data.table(temp)
ggplot(temp[variable == "Cumlift"], aes(x = bucket, y = value, fill = variable)) +
  geom_bar(stat = "identity") +
  ylab("Cumulative lift (Uplift") + 
  xlab("Bucket")
```

# Xgboost, custom grid tuning
```{r}
# Fit random forest: model
my_control = trainControl(method = "cv", number = 2, verboseIter = TRUE, 
                         summaryFunction = twoClassSummary, classProbs = TRUE)

xgb_model <- train(t_credito_new ~.,
                   data = train_full,
                   method = "xgbTree",
                   tuneLength = 1,
                   trControl = my_control
)

```
```{r}
# Print model to console
print(xgb_model)

# Plot model
plot(xgb_model)

#predict probabilities on testset
#type=”response” gives probabilities, type=”class” gives class
probs_xgb <- predict(object= xgb_model, newdata = test_x, type = "prob")


# plot ROC curve
roc_xgb <- roc(test_y$t_credito_new, probs_xgb$yes)
plot(roc_xgb)

# plot best threshold and accuracy
result_coords <- coords(roc_xgb, "best", best.method="closest.topleft",
                        ret=c("threshold", "accuracy"))
print(result_coords)#to get threshold and accuracy


## uplift and gains
# gets a table with relevant data
lift_data <- calculate_lift_table(depvar = ifelse(test_y$t_credito_new == "yes", 1, 0),
                                  predcol = probs_xgb$yes)

# print bars
temp <- lift_data %>% melt(id.vars = c("bucket"))
temp <- as.data.table(temp)
ggplot(temp[variable == "Cumlift"], aes(x = bucket, y = value, fill = variable)) +
  geom_bar(stat = "identity") +
  ylab("Cumulative lift (Uplift") + 
  xlab("Bucket")

```


* Calculate not the accuracy for the model, but the AUC or some measure
* Tune xgboost (first on small sample, at night for big)
* Make all this a function (for easy model evaluation)

```{r}

```




```{r}
#fit1_lasso <- glmnet(as.matrix(train_x), as.matrix(train_y), alpha = 1)
#fit1_lasso
#coef.glmnet()
#cv_out <- cv.glmnet(as.matrix(train_x), as.matrix(train_y), alpha=1,
#                   family= "binomial",type.measure = "mse" )
```
# glmnet regression
For unbalanced:
* decision trees
* penalized models (lassso,ridge, xgboost)
* get more data, and sub-sample keeping all those new positive observations
```{r}
# Create custom trainControl: myControl
#myControl <- trainControl(
  #method = "cv", number = 10,
  #summaryFunction = twoClassSummary,
  #classProbs = TRUE, # IMPORTANT!
  #verboseIter = TRUE
#)

# Fit glmnet model: model
#model <- train(
  #y ~., data = overfit,
  #method = "glmnet",
  #trControl = myControl
#)

# Print model to console
#print(model)

# Print maximum ROC statistic
#max(model[["results"]]["ROC"])
```







