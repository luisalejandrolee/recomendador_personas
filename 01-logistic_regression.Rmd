---
title: "Cool Title"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
---

Standard libraries
```{r}
rm(list = ls())
options(scipen=999) # scientific notation

library(data.table)
library(ggplot2)
library(caret)
library(plotly)
library(dplyr)
library(DT)
library(pander)
library(plotly)
library(xts)
library(reshape2)
library(arules)
library(glmnet)
library(mlbench)
library(pROC)
library(caTools)
library(ranger)
library(parallel)
library(doParallel)


#devtools::install_github("luisalejandrolee/utilucho")
library(utilucho)

#source("scripts/data_cleaning_and_preparation.R")

```

# Import

```{r}
# File names contain the sample size used. Need for the import (must match
# a run of the preparation file to save files with this sample)
#sample_size <- 5000
sample_size <- 100000
lags <- 2

folder <- "data/staging/"
file_dt <- paste0("master_n", sample_size , "_lags", max(lags), ".csv")
dt <- fread(paste0(folder, file_dt))

```
# Remove some variables
Due to collineatiry. Particularly, the "diff", "new" and " loss" which are already captured
by the "lag_1" variables. Careful here, because one must keep some "loss" or "new"
variables intended to be the target
```{r}
# target to keep (so exclude from general removal of variables below)
target_var <- "t_credito_new"

# manual removal. Key here to avoid the lag_1 of the target var (perfect collinearity)
cols_manual <- "t_credito_lag_1"

# remove all variables that by construction will probably have perfecto collinearity
# with other regressors
cols_diff <- grep('_diff', names(dt), value = TRUE)
cols_new <- grep('_new', names(dt), value = TRUE)
cols_loss <- grep('_loss', names(dt), value = TRUE)

# put all groups together, but avoid removing the target_var
cols_remove <- c(cols_diff, cols_new, cols_loss, cols_manual)
cols_remove <- cols_remove[!(cols_remove %in% target_var)]

# finally remove the chosen columns
dt[, (cols_remove) := NULL]

#cols_remove
#names(dt)
```


# Missing values

```{r}
# remove all na's
dt <- na.omit(dt)

```

# Train and test split

```{r}
# columns not to be included in the regression
all_cols <- names(dt)[!(names(dt) %in% c("V1", "periodo", "num_id"))]
# target variable
y_cols <- target_var
dt[, (y_cols) := lapply(.SD, as.factor), .SDcols = y_cols] # convert to factor

# all predictor variables (all except the target)
x_cols <- all_cols[!(all_cols %in% y_cols)]

#length(names(dt))
#length(all_cols)
#length(x_cols)
#length(y_cols)

# months to use as test
test_months <- c("2018-06-30")
# train months are all but the ones chosen for test
months <- unique(dt$periodo) # all months in data
train_months <- months[!(months %in% test_months)] # all months but for test

# notice that splits below are not done randomly. This is due to having time
# series variables. If done randomly, one would have the model observing data of
# future purchases to predict past ones (for example, client A in August for training,
# and same client A in July for testing)

# complete training and test set (including x and y), used for traditional logit regression
train_full <- dt[periodo %in% train_months, .SD, .SDcols = all_cols]
test_full <- dt[periodo %in% test_months, .SD, .SDcols = all_cols]

# separate predictors (x) vs target (y) for lasso (glmnet) regression
train_x <- dt[periodo %in% train_months, .SD, .SDcols = x_cols]
train_y <- dt[periodo %in% train_months, lapply(.SD, as.factor), .SDcols = y_cols]

# test split: x for explanatory, y for target
test_x <- dt[periodo %in% test_months, .SD, .SDcols = x_cols]
test_y <- dt[periodo %in% test_months, lapply(.SD, as.factor), .SDcols = y_cols]

#ncol(train_full)
#train_x
#train_y

#test_x
#test_y
```

```{r}
train_full[t_credito_new == "1", .(t_credito_new)]
train_full
```

# Logistic regression

## Fit model

```{r}

fit1_logit <- glm(data = train_full, t_credito_new ~., family = "binomial",
                  control = list(maxit = 50))

summary(fit1_logit)
```
## Confusion matrix, ROC curve and AUC
```{r}
#predict probabilities on testset
#type=”response” gives probabilities, type=”class” gives class
glm_prob <- predict.glm(object = fit1_logit, newdata = test_x, type = "response")
#summary(glm_prob)
#make classifications predictions based on probability

## threshold on probability to decide on class
glm_predict <- ifelse(glm_prob > 0.5, 1, 0)

# use caret to do both the confusion matrix and print some stats
confusionMatrix(as.factor(glm_predict), test_y$t_credito_new)

# use caTools package to plot ROC curve
colAUC(glm_predict, test_y$t_credito_new, plotROC = TRUE)


#test_y[t_credito_new==1]
#glm_predict
#glm_predict == test_y$t_credito_new
```
## Conclusions on traditional logistic

The model is highly unbalanced making the use of other models necessary.

The naive prediction (no one buys a credit card) gives over 99 percent accuracy 
(the "No information Rate" measure)

One could calculate the AUC, but is generally not so useful for just one model. The
AUC is more useful for comparing models, selecting among them when tuning parameters.
Hence, usually what is found for AUC uses caret's trainControl object, to try different
models.


Next steps:

* Try other models to deal with unbalanced class
  * LASSO and Random Forest (datacamp has the course in that same order)
  
  * Try cross validation (use trainControl) to select regularization term.
  I don't trust it much (since I have time series) but worth trying and learning from it

# Random forest

Notice that the logistic regression is simple in the sense that it doesn't have
any hyperparametres to tune. The next model would be a random forest, for which the
main parameter is "mtry" (number of explanatory variables to randomly try at each split 
of each tree). Caret chooses this parameter automatically. The other parameter is 
the length of the tree (but caret doesn't do it automatically)

## Simple cross validation, auto tuning
mtry is by default chosen in caret (tuneLength = 3  means try 3 different values
for mtry, and choose the best one)
```{r}
# needed to use the caret functions ( 0 and 1 are not possible variable names,
# and caret needs the levels in the factors to be possible variable names)
train_full$t_credito_new <- ifelse(train_full$t_credito_new == "0", "no", "yes")

# Fit random forest: model
## tuneLenght auto tunes default parameters for chosen model. For random forest,
## it tunes mtry (number of features randomly chosen for growing each tree)
## twoClassSummary indicates using AUC for model selection (vs. accuracy which is default)
randfor_model_default <- train(
  t_credito_new ~.,
  tuneLength = 2,
  data = train_full, method = "ranger",
  trControl = trainControl(method = "cv", number = 2, verboseIter = TRUE, 
                           summaryFunction = twoClassSummary, classProbs = TRUE)
)

# Print model to console
print(randfor_model_default)

# Plot model
plot(randfor_model_default)
```
## Save the model
```{r}
models_path <- "data/models/"
model_name <- "rforest_default_tuning_3.rds"
saveRDS(randfor_model_default, paste0(models_path, model_name))


#my_model <- readRDS("model.rds")
```

## Detailed cross validation, custom grid tuning
tuneLength allows auto tuning of the parameters with chosen values by the caret package creator.
tuneGrid allows specifying the values for each parameter (but also can only tune some 
parameters, not all). For random forest, only mtry can be tuned
```{r}
# do paralell computing (use all cores but one)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

start_time <- Sys.time()

# Fit random forest: model
randfor_model_custom <- train(
  t_credito_new ~.,
  tuneGrid = data.frame(mtry = c(2)),
  data = train_full, method = "ranger",
  trControl = trainControl(method = "cv", number = 2, verboseIter = TRUE, 
                           summaryFunction = twoClassSummary, classProbs = TRUE,
                           allowParallel = TRUE)
)

end_time <- Sys.time()
print(paste0("Total time running = ", end_time - start_time))

# go back to non-parallel processing
stopCluster(cluster) # stop the cluster
registerDoSEQ() # force R to go back to single thread processing

# Print model to console
print(randfor_model_custom)

# Plot model
plot(randfor_model_custom)
```

```{r}
#fit1_lasso <- glmnet(as.matrix(train_x), as.matrix(train_y), alpha = 1)
#fit1_lasso
#coef.glmnet()
#cv_out <- cv.glmnet(as.matrix(train_x), as.matrix(train_y), alpha=1,
#                   family= "binomial",type.measure = "mse" )
```
# glmnet regression
For unbalanced:
* decision trees
* penalized models (lassso,ridge, xgboost)
* get more data, and sub-sample keeping all those new positive observations
```{r}
# Create custom trainControl: myControl
#myControl <- trainControl(
  #method = "cv", number = 10,
  #summaryFunction = twoClassSummary,
  #classProbs = TRUE, # IMPORTANT!
  #verboseIter = TRUE
#)

# Fit glmnet model: model
#model <- train(
  #y ~., data = overfit,
  #method = "glmnet",
  #trControl = myControl
#)

# Print model to console
#print(model)

# Print maximum ROC statistic
#max(model[["results"]]["ROC"])
```



