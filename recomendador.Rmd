---
title: "Cool Title"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
---

Standard libraries
```{r}
rm(list = ls())
options(scipen=999) # scientific notation

library(data.table)
library(ggplot2)
library(caret)
library(plotly)
library(dplyr)
library(DT)
library(pander)
library(plotly)
library(xts)

#devtools::install_github("luisalejandrolee/utilucho")
library(utilucho)

source("scripts/data_cleaning_and_preparation.R")

```

# Import

```{r}
folder <- "data/raw/"

files <- c("mes_201706", "mes_201707", "mes_201708", "mes_201709", "mes_201710",
           "mes_201711", "mes_201712", "mes_201801", "mes_201802", "mes_201803",
           "mes_201804", "mes_201805", "mes_201806")

all_months <- import_as_list(folder, files)

```

# Clean data (first stage)
```{r}

cols_to_del <- c("TIPO_ID", "tipo_id2", "_NAME_")

# Make ID as character, remove columns, change PERIODO to date format,
# and format column names
clean_dt_in_list(all_months, cols_to_del)

```

```{r}
all_months[[10]]
```

# Sample data and join all months

```{r}
# Sample by choosing n clients from the last month, and then retrieve those for
# previous months

dts_sampled <- get_samples_dts(all_months, 100000, frac = FALSE) # sample data

dt <- rbindlist(dts_sampled) # merge all SAMPLED months in a single data.table
#dt <- rbindlist(all_months) # merge all NON SAMPLED months in a single data.table

#rm(all_months) # delete non merged datasets
#rm(dts_sampled)
# Check how many clients of the original sample remain month after month

#for(i in 1:length(dts_sampled)){
#  print(nrow(dts_sampled[[i]]))
#}
#dts_sampled[[2]]

dt
```



# Clean data (second stage)
## Check duplicates
```{r}
# check how many duplicates per month
dup <- dt[, .(num_id, duplicated(num_id)), by = periodo][V2 == TRUE]
dup[, .N, by = periodo]
```
## Remove duplicates
```{r}
# remove duplicate clients in the same month
dup <- dt[, duplicated(num_id), by = periodo] # keeps in V1 if duplicated
dt <- dt[dup$V1 == FALSE] # keep non-duplicated
rm(dup) # delete duplicates (not needed anymore)
str(dt)
```

# Feature engineering
* subtract t_cred_l1 - t_cred
* make it binary (1 if >0) and that's the target.

* Repeat the same for all desired products and lags (make a function)
* Also do without the binary addition, and that's the main feature engineering

For data exploration: 
* Count number of clients that **aqcuired** (bought) each product
* Count number of clients that **had** (possesion) each product
* Put all of it in a matrix of counts (with many columns). That's the input for
the Shiny app

```{r}
# required to do proper lag (as calendar time, not reported time)
dt <- complete_id_time(dt, "periodo", "num_id")

#dt[order(num_id, periodo), .N, by = num_id] %>% summary()
#dt[order(num_id, periodo)]
```




## Create lags

```{r}


#products <- c("t_credito", "crediservice")
products <- names(dt)[3:length(names(dt))] # all variables (except id and periodo)
lags <- c(1:(length(unique(dt$periodo))-1)) # all possible lags but last one

dt <- create_lagged_vars(dt, products, lags)
```
## Create acquisition and loss of products


```{r}

```

```{r}
# replace nas with zeros (create function. Also make some functions to give give median, etc)



# create lag variables for each (create function)
# create adquisition variables: when there's a new product (as a dummy) and when dropping one
# create lags for adquisition variables as well



# Check and analyiza data

## check values for each product: max number of products per client
## decide if making an upper bound
## check some stats (create shiny app!.. for time series)
## check correlations
## do a market basket analysis!

# are there products usually bought together?
# does it usually happen that when buying a credit card, another product is always owned?

```

























